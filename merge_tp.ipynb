{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving conversion of megatron checkpoint to huggingface transformer checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407be1ffbeb042d59836e22fce1c9264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to suppress warnings from megatron torch.load\n",
    "# you don't actually need megatron installed, but it needs the package to deserialize the model\n",
    "# this was done on a mac\n",
    "\n",
    "# sys.path.append(\"PATH TO MEGATRON\") uncommented as running in directory\n",
    "\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device_map = \"cpu\", torch_dtype = \"bfloat16\", low_cpu_mem_usage = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"../megatron-llama3-8B-PP4-TP8-mcore/iter_0000001\"\n",
    "# to get this you need to convert the llama model to megatron format first\n",
    "FILE_FORMAT = \"mp_rank_{tensor:02d}_{pipeline:03d}/model_optim_rng.pt\"\n",
    "\n",
    "# check exists:\n",
    "assert os.path.exists(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16128, 4096])\n",
      "embedding.shape=torch.Size([129024, 4096])\n"
     ]
    }
   ],
   "source": [
    "# usually resides first pipeline\n",
    "embedding = []\n",
    "for i in range(8):\n",
    "    to_add = torch.load(f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\")['model']['embedding.word_embeddings.weight']\n",
    "    if i==0:\n",
    "        print(to_add.shape)\n",
    "    embedding.append(to_add)\n",
    "\n",
    "embedding = torch.cat(embedding, dim=0)\n",
    "print(f\"{embedding.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All duplicates after x: True\n",
      "Is equal to huggingface model: True\n"
     ]
    }
   ],
   "source": [
    "# megatron creates additional embed weights\n",
    "\n",
    "x, y = reference_model.model.embed_tokens.weight.data.shape\n",
    "print(\"All duplicates after x: \", end =\"\")\n",
    "print(\n",
    "    all(\n",
    "        [torch.equal(embedding[x], _x ) for _x in embedding[x:, :]]\n",
    "        )\n",
    ")\n",
    "embedding = embedding[:x,:]\n",
    "print(\"Is equal to huggingface model: \", end=\"\")\n",
    "print(torch.equal(embedding, reference_model.model.embed_tokens.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper variables\n",
    "hidden_size = reference_model.config.hidden_size\n",
    "num_heads = reference_model.config.num_attention_heads\n",
    "n_layers = reference_model.config.num_hidden_layers\n",
    "gqa_head = reference_model.config.num_key_value_heads\n",
    "ffn_size = reference_model.config.intermediate_size\n",
    "dim = hidden_size // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_to_qkv(fused, nh, ng, dim):\n",
    "    \"\"\"\n",
    "    split fused qkv into q, k, v\n",
    "\n",
    "    Args:\n",
    "        fused: [b, s, dim*3*nh]\n",
    "        nh: number of heads\n",
    "        ng: number of groups\n",
    "        dim: kv channels\n",
    "\n",
    "    Returns:\n",
    "        q,k,v\n",
    "    \"\"\"\n",
    "    hidden_size = dim * nh\n",
    "    reshaped = fused.reshape(ng, dim*nh//ng + 2 *dim, -1)\n",
    "    q,k,v = torch.split(reshaped, [dim*nh//ng, dim, dim], dim=1)\n",
    "    return q.reshape(-1, hidden_size), k.reshape(-1, hidden_size), v.reshape(-1, hidden_size)\n",
    "\n",
    "def fused_mlp_to_gate_up(fused_tensor,ffn_hidden_size):\n",
    "    \"\"\"\n",
    "    Spit into gate and up_proj\n",
    "\n",
    "    Returns:\n",
    "        gate, up_proj\n",
    "    \"\"\"\n",
    "    gate, up_proj = torch.split(fused_tensor, [ffn_hidden_size, ffn_hidden_size], dim=0)\n",
    "    return gate, up_proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n",
      "torch.Size([768, 4096])\n"
     ]
    }
   ],
   "source": [
    "# load one attention layer\n",
    "attn = []\n",
    "for i in range(8):\n",
    "\n",
    "    to_add = torch.load(f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\")['model']['decoder.layers.0.self_attention.linear_qkv.weight']\n",
    "    print(to_add.shape)\n",
    "    attn.append(to_add)\n",
    "\n",
    "attn = torch.cat(attn, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "q,k,v = fused_to_qkv(attn, num_heads, gqa_head, dim)\n",
    "print(torch.equal(q, reference_model.model.layers[0].self_attn.q_proj.weight.data))\n",
    "print(torch.equal(k, reference_model.model.layers[0].self_attn.k_proj.weight.data))\n",
    "print(torch.equal(v, reference_model.model.layers[0].self_attn.v_proj.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "# linear proj -> o_proj\n",
    "o_proj = []\n",
    "for i in range(8):\n",
    "    to_add = torch.load(f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\")['model']['decoder.layers.0.self_attention.linear_proj.weight']\n",
    "    o_proj.append(to_add)\n",
    "\n",
    "o_proj = torch.cat(o_proj, dim=1)\n",
    "print(o_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(o_proj, reference_model.model.layers[0].self_attn.o_proj.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FC1 -> up_proj, date_proj\n",
    "\n",
    "Megatron saves FC1 as gate_tp1,up_tp1, gate_tp2, up_tp2 ... gate_tpN, up_tpN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_proj.shape=torch.Size([14336, 4096])\n",
      "up_proj.shape=torch.Size([14336, 4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gate_proj = []\n",
    "up_proj = []\n",
    "for i in range(8):\n",
    "    path = f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\"\n",
    "    fused = torch.load(path)['model']['decoder.layers.0.mlp.linear_fc1.weight']\n",
    "    _gate_proj, _up_proj = fused_mlp_to_gate_up(fused, ffn_size // 8)\n",
    "    gate_proj.append(_gate_proj)\n",
    "    up_proj.append(_up_proj)\n",
    "\n",
    "gate_proj = torch.cat(gate_proj, dim=0)\n",
    "up_proj = torch.cat(up_proj, dim=0)\n",
    "print(f\"{gate_proj.shape=}\")\n",
    "print(f\"{up_proj.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(gate_proj, reference_model.model.layers[0].mlp.gate_proj.weight.data))\n",
    "print(torch.equal(up_proj, reference_model.model.layers[0].mlp.up_proj.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC2 -> down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_proj.shape=torch.Size([4096, 14336])\n"
     ]
    }
   ],
   "source": [
    "down_proj = []\n",
    "for i in range(8):\n",
    "    path = f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\"\n",
    "    to_add = torch.load(path)['model']['decoder.layers.0.mlp.linear_fc2.weight']\n",
    "    down_proj.append(to_add)\n",
    "\n",
    "down_proj = torch.cat(down_proj, dim=1)\n",
    "print(f\"{down_proj.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(down_proj, reference_model.model.layers[0].mlp.down_proj.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "This was a little confusing but qkv_norm is referring to layer norm before qkv => input_layernorm\n",
    "qkv_norm -> input_layernorm\n",
    "\n",
    "post_attention is fc1_layernorm\n",
    "\n",
    "Norms are duplicated across all tensors parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "input_layernorm = []\n",
    "for i in range(8):\n",
    "    path = f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\"\n",
    "    to_add = torch.load(path)['model']['decoder.layers.0.self_attention.linear_qkv.layer_norm_weight']\n",
    "    print(to_add.shape)\n",
    "    input_layernorm.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All duplicate weights: True\n",
      "\n",
      "Reference to HF: True\n"
     ]
    }
   ],
   "source": [
    "# verify that they are all duplicates\n",
    "# should be trivial if x=y and x=z then y=z\n",
    "print(\"All duplicate weights: \", end = '')\n",
    "print(all([torch.equal(input_layernorm[0], x) for x in input_layernorm]))\n",
    "\n",
    "print(\"\\nReference to HF: \", end = '')\n",
    "print(torch.equal(input_layernorm[0], reference_model.model.layers[0].input_layernorm.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "post_attention_layernorm = []\n",
    "for i in range(8):\n",
    "    path = f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 0)}\"\n",
    "    to_add = torch.load(path)['model']['decoder.layers.0.mlp.linear_fc1.layer_norm_weight']\n",
    "    print(to_add.shape)\n",
    "    post_attention_layernorm.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All duplicate weights: True\n",
      "\n",
      "Reference to HF: True\n"
     ]
    }
   ],
   "source": [
    "# verify that they are all duplicates\n",
    "print(\"All duplicate weights: \", end = '')\n",
    "print(all([torch.equal(post_attention_layernorm[0], x) for x in post_attention_layernorm]))\n",
    "\n",
    "print(\"\\nReference to HF: \", end = '')\n",
    "print(torch.equal(post_attention_layernorm[0], reference_model.model.layers[0].post_attention_layernorm.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/43y1p0rx537g7c875p4mb63r0000gn/T/ipykernel_97848/3138036287.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  to_add = torch.load(f\"{BASE_FOLDER}/{FILE_FORMAT.format(i, 3)}\")['model']['decoder.final_layernorm.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "final_layernorm = []\n",
    "for i in range(8):\n",
    "    # note using final pipeline\n",
    "    to_add = torch.load(f\"{BASE_FOLDER}/{FILE_FORMAT.format(i, 3)}\")['model']['decoder.final_layernorm.weight']\n",
    "    print(to_add.shape)\n",
    "    final_layernorm.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All duplicate weights: True\n",
      "\n",
      "Reference to HF: True\n"
     ]
    }
   ],
   "source": [
    "# verify that they are all duplicates\n",
    "print(\"All duplicate weights: \", end = '')\n",
    "print(all([torch.equal(final_layernorm[0], x) for x in final_layernorm]))\n",
    "\n",
    "print(\"\\nReference to HF: \", end = '')\n",
    "print(torch.equal(final_layernorm[0], reference_model.model.norm.weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "torch.Size([16128, 4096])\n",
      "output_layer.shape=torch.Size([129024, 4096])\n"
     ]
    }
   ],
   "source": [
    "output_layer = []\n",
    "# note that output layer in pipeline 3\n",
    "for i in range(8):\n",
    "    path = f\"{BASE_FOLDER}/{FILE_FORMAT.format(tensor = i, pipeline = 3)}\"\n",
    "    to_add = torch.load(path)['model']['output_layer.weight']\n",
    "    print(to_add.shape)\n",
    "    output_layer.append(to_add)\n",
    "\n",
    "output_layer = torch.cat(output_layer, dim=0)\n",
    "print(f\"{output_layer.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
